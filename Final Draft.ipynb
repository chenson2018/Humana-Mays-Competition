{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Preperation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from itertools import chain\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cgn31\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3057: DtypeWarning: Columns (8) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "training = pd.read_csv('HMAHCC_COMP.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataPrep:\n",
    "    def __init__(self, df):\n",
    "        self.raw_df = df\n",
    "    \n",
    "    def seperate_frame(self):\n",
    "        new_diagnosis_col = ['New diagnosis - CPD',\n",
    "                         'New diagnosis - Hypertension', \n",
    "                         'New diagnosis - Top 5', \n",
    "                         'New diagnosis - CAD', \n",
    "                         'New diagnosis - CHF', \n",
    "                         'New diagnosis - Diabetes']\n",
    "\n",
    "        call_col = ['Inbound Call by Prov',\n",
    "                    'Inbound Call by Mbr',\n",
    "                    'Inbound Call by Other']\n",
    "\n",
    "        non_rx_claim_col = ['Surgery',\n",
    "                            'Fully Paid Claim']\n",
    "\n",
    "        rx_claim_col = ['RX Claim - New Drug',\n",
    "                        'RX Claim - First Time Mail Order']\n",
    "        \n",
    "        self.new_diagnosis = self.raw_df[self.raw_df['event_descr'].isin(new_diagnosis_col)]\n",
    "\n",
    "        self.new_diagnosis = self.new_diagnosis.drop(['event_attr6',\n",
    "                                                      'event_attr7',\n",
    "                                                      'event_attr8',\n",
    "                                                      'event_attr9',\n",
    "                                                      'event_attr10',\n",
    "                                                      'PAY_DAY_SUPPLY_CNT',\n",
    "                                                      'PAYABLE_QTY',\n",
    "                                                      'MME',\n",
    "                                                      'DRUG_TYPE',\n",
    "                                                      'Specialty',\n",
    "                                                      'Specialty2',\n",
    "                                                      'Specialty3'],\n",
    "                                                       axis = 1)\n",
    "\n",
    "        self.new_diagnosis.columns = ['id',\n",
    "                                 'event_descr',\n",
    "                                 'diagnosis',\n",
    "                                 'place_of_treatment',\n",
    "                                 'charge_amount',\n",
    "                                 'net_paid_amount',\n",
    "                                 'member_responsible_amount',\n",
    "                                 'Days']\n",
    "        \n",
    "        self.call = self.raw_df[self.raw_df['event_descr'].isin(call_col)]\n",
    "\n",
    "        self.call = self.call.drop(['event_attr6',\n",
    "                          'event_attr7',\n",
    "                          'event_attr8',\n",
    "                          'event_attr9',\n",
    "                          'event_attr10',\n",
    "                          'PAY_DAY_SUPPLY_CNT',\n",
    "                          'PAYABLE_QTY',\n",
    "                          'MME',\n",
    "                          'DRUG_TYPE',\n",
    "                          'Specialty',\n",
    "                          'Specialty2',\n",
    "                          'Specialty3'],\n",
    "                           axis = 1)\n",
    "\n",
    "        self.call.columns = ['id',\n",
    "                        'event_descr',\n",
    "                        'call_category',\n",
    "                        'inquiry_reason_description',\n",
    "                        'disposition_description',\n",
    "                        'origin',\n",
    "                        'location',\n",
    "                        'Days']\n",
    "        \n",
    "        self.non_rx_claim = self.raw_df[self.raw_df['event_descr'].isin(non_rx_claim_col)]\n",
    "\n",
    "        self.non_rx_claim = self.non_rx_claim.drop(['event_attr6',\n",
    "                                          'event_attr7',\n",
    "                                          'event_attr8',\n",
    "                                          'event_attr9',\n",
    "                                          'event_attr10',\n",
    "                                          'PAY_DAY_SUPPLY_CNT',\n",
    "                                          'PAYABLE_QTY',\n",
    "                                          'MME',\n",
    "                                          'DRUG_TYPE',\n",
    "                                          'Specialty',\n",
    "                                          'Specialty2',\n",
    "                                          'Specialty3'],\n",
    "                                           axis = 1)\n",
    "\n",
    "        self.non_rx_claim.columns = ['id',\n",
    "                                'event_descr',\n",
    "                                'diagnosis',\n",
    "                                'place_of_treatment',\n",
    "                                'charge_amount',\n",
    "                                'net_paid_amount',\n",
    "                                'member_responsible_amount',\n",
    "                                'Days']\n",
    "        \n",
    "        self.new_provider = self.raw_df[self.raw_df['event_descr']=='New provider']\n",
    "        self.new_provider = self.new_provider.iloc[:,:2]\n",
    "        \n",
    "        self.rx_claim = self.raw_df[self.raw_df['event_descr'].isin(rx_claim_col)]\n",
    "\n",
    "        self.rx_claim = self.rx_claim.drop(['event_attr6',\n",
    "                          'event_attr7'],\n",
    "                           axis = 1)\n",
    "\n",
    "        self.rx_claim.columns = ['id',\n",
    "                            'event_descr',\n",
    "                            'gpi_drug_group6_id',\n",
    "                            'gpi_drug_class_description',\n",
    "                            'brand_name',\n",
    "                            'drug_group_id',\n",
    "                            'generic_name',\n",
    "                            'drug_group_description',\n",
    "                            'member_responsible_amount',\n",
    "                            'gpi_drug_group8_id',\n",
    "                            'Days',\n",
    "                            'PAY_DAY_SUPPLY_CNT',\n",
    "                            'PAYABLE_QTY',\n",
    "                            'MME',\n",
    "                            'DRUG_TYPE',\n",
    "                            'Specialty',\n",
    "                            'Specialty2',\n",
    "                            'Specialty3']\n",
    "        \n",
    "        self.rx_paid = self.raw_df[self.raw_df['event_descr']=='RX Claim - Paid']\n",
    "\n",
    "        self.rx_paid = self.rx_paid.drop(['event_attr2',\n",
    "                                'event_attr7'],\n",
    "                                 axis = 1)\n",
    "\n",
    "        self.rx_paid.columns = ['id',\n",
    "                           'event_descr',\n",
    "                           'gpi_drug_class_description',\n",
    "                           'rx_cost',\n",
    "                           'net_paid_amount',\n",
    "                           'brand_name',\n",
    "                           'drug_group_description',\n",
    "                           'generic_name',\n",
    "                           'member_responsible_amount',\n",
    "                           'gpi_drug_group8_id',\n",
    "                           'Days',\n",
    "                           'PAY_DAY_SUPPLY_CNT',\n",
    "                           'PAYABLE_QTY',\n",
    "                           'MME',\n",
    "                           'DRUG_TYPE',\n",
    "                           'Specialty',\n",
    "                           'Specialty2',\n",
    "                           'Specialty3']\n",
    "        \n",
    "        self.rx_reject = self.raw_df[self.raw_df['event_descr']=='RX Claim - Rejected']\n",
    "\n",
    "        self.rx_reject = self.rx_reject.drop(['PAY_DAY_SUPPLY_CNT',\n",
    "                                    'PAYABLE_QTY',\n",
    "                                    'MME',\n",
    "                                    'DRUG_TYPE',\n",
    "                                    'Specialty',\n",
    "                                    'Specialty2',\n",
    "                                    'Specialty3'],\n",
    "                                     axis =1)\n",
    "\n",
    "        self.rx_reject.columns = ['id',\n",
    "                             'event_descr',\n",
    "                             'status_code',\n",
    "                             'diagnosis',\n",
    "                             'cob',\n",
    "                             'claim_tier',\n",
    "                             'brand_name',\n",
    "                             'generic_name',\n",
    "                             'ndc_id',\n",
    "                             'pay_day_supply_count',\n",
    "                             'member_responsible_amount',\n",
    "                             'gpi_drug_group8_id',\n",
    "                             'Days']\n",
    "        \n",
    "    def get_opioid_data(self):\n",
    "        self.opioid_data = self.raw_df[self.raw_df['PAY_DAY_SUPPLY_CNT'].notnull()]\n",
    "        self.opioid_data_grouped = self.opioid_data.groupby(by='id')\n",
    "        \n",
    "    def LTOT(self):\n",
    "        \n",
    "        def get_LTOT(ID):\n",
    "            try:\n",
    "                group = self.opioid_data_grouped.get_group(ID)\n",
    "                frame = group[['Days', 'PAY_DAY_SUPPLY_CNT']].drop_duplicates()\n",
    "                frame = frame[frame['Days']>=0]\n",
    "                frame['drugs until'] = (frame['Days'] + frame['PAY_DAY_SUPPLY_CNT']).astype(int)\n",
    "                frame['range'] = frame.apply(lambda x : range(x['Days'].astype(int),x['drugs until'].astype(int)),1)\n",
    "\n",
    "                concat = concatenated = chain(*list(frame['range']))\n",
    "                concat = set(concat)\n",
    "\n",
    "                day_frame = pd.DataFrame(columns = ['Days', 'Has Drug?'])\n",
    "                day_frame['Days'] = range(max(concat)+1)\n",
    "                day_frame['Has Drug?'] = (day_frame['Days'].isin(concat))\n",
    "\n",
    "                for n in range(180,len(day_frame)+1):\n",
    "                    frame_slice = day_frame.iloc[n-180:n]\n",
    "                    drug_days = np.sum(frame_slice['Has Drug?'])\n",
    "\n",
    "                    if drug_days >= 162:\n",
    "                        return (True, n-180, n)\n",
    "\n",
    "                return (False, np.nan, np.nan)\n",
    "            except:\n",
    "                return (np.nan, np.nan, np.nan)\n",
    "        \n",
    "        id_list = self.raw_df['id'].drop_duplicates().values\n",
    "        \n",
    "        response_variable = pd.DataFrame(id_list, columns = ['id'])\n",
    "        response_variable['LTOT'] = response_variable['id'].map(get_LTOT)\n",
    "        \n",
    "        response_variable[['LTOT', 'Begining Date', 'End Date']] = \\\n",
    "        pd.DataFrame(response_variable['LTOT'].tolist(), index = response_variable.index)\n",
    "        \n",
    "        self.response_variable = response_variable.set_index('id')\n",
    "    \n",
    "    \n",
    "    def main_feature_extraction(self): \n",
    "        opioid_all_time = self.rx_paid[self.rx_paid['PAY_DAY_SUPPLY_CNT'].notnull()]['generic_name'].value_counts()\n",
    "        mask = self.rx_paid['generic_name'].map(lambda x: x in opioid_all_time.index.values)\n",
    "        true_opioid = self.rx_paid[mask]\n",
    "        \n",
    "        opioid_grouped = true_opioid.groupby(by=['id'])\n",
    "\n",
    "        idtestlist = true_opioid['id'].drop_duplicates()\n",
    "        features3 = pd.DataFrame()\n",
    "\n",
    "        for ID in idtestlist:\n",
    "            tmp = opioid_grouped.get_group(ID)\n",
    "\n",
    "            # MME (per day) on day 0\n",
    "            # Suuply_CNT on day 0\n",
    "            on_day0 = tmp[tmp['Days'] == 0] \n",
    "            if not on_day0.empty:\n",
    "                MME0 = on_day0['MME'].values[0]\n",
    "                SC0 = on_day0['PAY_DAY_SUPPLY_CNT'].values[0]\n",
    "                PQ0 = on_day0['PAYABLE_QTY'].values[0]\n",
    "            else:\n",
    "                MME0 = 0\n",
    "                SC0 = 0\n",
    "                PQ0 = 0\n",
    "\n",
    "            # max MME (per day) prior to day 0\n",
    "            # average MME (per day) prior to day 0\n",
    "            # Total Supply_CNT prior to day 0\n",
    "            prior_day0 = tmp[tmp['Days'] < 0]\n",
    "            if not prior_day0.empty:\n",
    "                maxMME = np.nanmax(prior_day0['MME'].values)\n",
    "                avgMME = np.nanmean(prior_day0['MME'].values)\n",
    "                totalSC = np.nansum(prior_day0['PAY_DAY_SUPPLY_CNT'].values)\n",
    "                totalPQ = np.nansum(prior_day0['PAYABLE_QTY'].values)\n",
    "            else:\n",
    "                maxMME = 0\n",
    "                avgMME = 0\n",
    "                totalSC = 0\n",
    "                totalPQ = 0\n",
    "\n",
    "            output = pd.DataFrame({'MME_on_day0': MME0, \n",
    "                                 'SUPPLY_CNT_on_day0': SC0,\n",
    "                                   'PAYABLE_QTY_on_day0': PQ0,\n",
    "                                 'max_MME_prior': maxMME,\n",
    "                                 'avg_MME_prior': avgMME,\n",
    "                                 'total_SUPPLY_CNT_prior': totalSC,\n",
    "                                  'total_PAYABLE_QTY_prior': totalPQ},\n",
    "                                  index = [ID])\n",
    "\n",
    "            features3 = features3.append(output, sort=False)\n",
    "\n",
    "        # MME_on_day0, max_MME_prior, avg_MME_prior has some missing value, fill with medians\n",
    "        features3['MME_on_day0'] = features3['MME_on_day0'].fillna(np.nanmedian(features3['MME_on_day0']))\n",
    "        features3['max_MME_prior'] = features3['max_MME_prior'].fillna(np.nanmedian(features3['max_MME_prior']))\n",
    "        features3['avg_MME_prior'] = features3['avg_MME_prior'].fillna(np.nanmedian(features3['avg_MME_prior']))\n",
    "\n",
    "        # add one more feature: supply_times\n",
    "        supply_times = true_opioid[true_opioid['Days']<=0].groupby(by=['id'])['PAY_DAY_SUPPLY_CNT'].count()\n",
    "        supply_times = pd.DataFrame(supply_times)\n",
    "        supply_times.columns = ['supply_times']\n",
    "        features3 = features3.merge(supply_times, left_on=features3.index.values, right_on=supply_times.index.values)\n",
    "        features3 = features3.set_index('key_0')\n",
    "        \n",
    "        self.main_features = features3\n",
    "        \n",
    "    def generic_feature_extraction(self):\n",
    "        opioid_all_time = self.rx_paid[self.rx_paid['PAY_DAY_SUPPLY_CNT'].notnull()]['generic_name'].value_counts()\n",
    "        mask = self.rx_paid['generic_name'].map(lambda x: x in opioid_all_time.index.values)\n",
    "        true_opioid = self.rx_paid[mask] \n",
    "        \n",
    "        opioid2_grouped = true_opioid[true_opioid['Days'] == 0].groupby(by=['id'])\n",
    "\n",
    "        idtestlist = true_opioid[true_opioid['Days'] == 0]['id'].drop_duplicates()\n",
    "\n",
    "        ## those commented-out codes are used to get other entry values\n",
    "        # def product_sum(df):\n",
    "        #     return(df['MME'].values.dot(df['PAY_DAY_SUPPLY_CNT']e.values))\n",
    "\n",
    "        features = pd.DataFrame()\n",
    "        for ID in idtestlist:\n",
    "            tmp = opioid2_grouped.get_group(ID)\n",
    "            output = pd.DataFrame(tmp.groupby(by='generic_name')['PAY_DAY_SUPPLY_CNT'].agg(np.nansum)).T\n",
    "        #     output = output.iloc[0:1,:]\n",
    "            output.index = [ID]\n",
    "            # features = pd.concat([output, features], axis=1, sort=False)\n",
    "            features = features.append(output, sort=False)\n",
    "\n",
    "        features = features.fillna(0)\n",
    "        \n",
    "        pca = PCA(n_components = 10) \n",
    "        X10D = pca.fit_transform(features)\n",
    "        \n",
    "        features_matthew_generic = pd.DataFrame(X10D, index=features.index.values)\n",
    "        features_matthew_generic.columns = ['generic_pc{}'.format(x) for x in features_matthew_generic.columns]\n",
    "        \n",
    "        self.generic_features = features_matthew_generic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cgn31\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:249: RuntimeWarning: All-NaN slice encountered\n",
      "C:\\Users\\cgn31\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:250: RuntimeWarning: Mean of empty slice\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 43min 57s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "training_class = DataPrep(training)\n",
    "training_class.seperate_frame()\n",
    "training_class.get_opioid_data()\n",
    "training_class.LTOT()\n",
    "training_class.main_feature_extraction()\n",
    "training_class.generic_feature_extraction()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next we actually run the models!!!\n",
    "(this will be made into a class soon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "final_frame = pd.concat([training_class.response_variable[['LTOT']], \n",
    "                          training_class.main_features, \n",
    "                          training_class.generic_features], \n",
    "                          axis=1, join = 'inner').dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = final_frame.iloc[:,1:]\n",
    "y = final_frame.iloc[:,0].map(lambda x: 1 if x == True else 0 )\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=33)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import cross_validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train roc_auc:  0.865347253257638\n",
      "test roc_auc:  0.8796345743057674\n",
      "Wall time: 19.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "rf_model = RandomForestClassifier(n_estimators=300, \n",
    "                                  max_depth=9,\n",
    "                                  random_state=100)\n",
    "\n",
    "cv_results = cross_validate(rf_model, X_train, y_train, cv=3,\n",
    "                            scoring= 'roc_auc',\n",
    "                            return_train_score=True,\n",
    "                            return_estimator =True)\n",
    "\n",
    "print('train roc_auc: ', np.mean(cv_results['test_score']))\n",
    "\n",
    "rf_model.fit(X_train, y_train)\n",
    "prediction_on_X_test = rf_model.predict_proba(X_test)\n",
    "print('test roc_auc: ', roc_auc_score(y_test, prediction_on_X_test[:,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural Net:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X)\n",
    "X_scale = scaler.transform(X)\n",
    "y_array = y.values\n",
    "X_train, X_test_s, y_train, y_test = train_test_split(X_scale, y_array, test_size=0.33, random_state=33)\n",
    "X_train_mlp, X_validation, y_train_mlp, y_validation = train_test_split(X_train, y_train, test_size=0.15, random_state=33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\cgn31\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\cgn31\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\cgn31\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4115: The name tf.random_normal is deprecated. Please use tf.random.normal instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\cgn31\\Anaconda3\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\cgn31\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3376: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\cgn31\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From C:\\Users\\cgn31\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.45808, saving model to best_model2.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.45808 to 0.44879, saving model to best_model2.h5\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.44879 to 0.44844, saving model to best_model2.h5\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.44844 to 0.44812, saving model to best_model2.h5\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.44812\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.44812 to 0.44430, saving model to best_model2.h5\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.44430 to 0.44394, saving model to best_model2.h5\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.44394\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.44394\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.44394 to 0.44114, saving model to best_model2.h5\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.44114\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.44114\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.44114\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.44114\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.44114\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.44114\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.44114\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.44114 to 0.44003, saving model to best_model2.h5\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.44003\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.44003 to 0.43933, saving model to best_model2.h5\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.43933\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.43933\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.43933\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.43933\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.43933\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.43933\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.43933\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.43933\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.43933 to 0.43771, saving model to best_model2.h5\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.43771\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.43771\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.43771 to 0.43751, saving model to best_model2.h5\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.43751\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.43751\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.43751\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.43751\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.43751\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.43751 to 0.43738, saving model to best_model2.h5\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.43738\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.43738\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.43738\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.43738\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.43738\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.43738\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.43738\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.43738\n",
      "\n",
      "Epoch 00047: val_loss improved from 0.43738 to 0.43729, saving model to best_model2.h5\n",
      "\n",
      "Epoch 00048: val_loss improved from 0.43729 to 0.43634, saving model to best_model2.h5\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.43634\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.43634\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.43634\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.43634\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.43634\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.43634\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.43634\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.43634\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.43634\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.43634\n",
      "\n",
      "Epoch 00059: val_loss improved from 0.43634 to 0.43609, saving model to best_model2.h5\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.43609\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.43609\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.43609\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.43609\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.43609\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.43609\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.43609\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.43609\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.43609\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.43609\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.43609\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.43609\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.43609\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.43609\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.43609\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.43609\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.43609\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.43609\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.43609\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.43609\n",
      "\n",
      "Epoch 00080: val_loss improved from 0.43609 to 0.43603, saving model to best_model2.h5\n",
      "\n",
      "Epoch 00081: val_loss improved from 0.43603 to 0.43510, saving model to best_model2.h5\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.43510\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.43510\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.43510\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.43510\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.43510\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.43510\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.43510\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.43510\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.43510\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.43510\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.43510\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.43510\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.43510\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.43510\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.43510\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.43510\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.43510\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.43510\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.43510\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 0.43510\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 0.43510\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 0.43510\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 0.43510\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 0.43510\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 0.43510\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 0.43510\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 0.43510\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 0.43510\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 0.43510\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 0.43510\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 0.43510\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 0.43510\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 0.43510\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 0.43510\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 0.43510\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 0.43510\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 0.43510\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00119: val_loss did not improve from 0.43510\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 0.43510\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 0.43510\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 0.43510\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 0.43510\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 0.43510\n",
      "\n",
      "Epoch 00125: val_loss did not improve from 0.43510\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 0.43510\n",
      "\n",
      "Epoch 00127: val_loss did not improve from 0.43510\n",
      "\n",
      "Epoch 00128: val_loss did not improve from 0.43510\n",
      "\n",
      "Epoch 00129: val_loss did not improve from 0.43510\n",
      "\n",
      "Epoch 00130: val_loss did not improve from 0.43510\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 0.43510\n",
      "Epoch 00131: early stopping\n",
      "Train: 0.810, Test: 0.499\n",
      "Wall time: 1min 56s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = Sequential()\n",
    "\n",
    "#First Hidden Layer\n",
    "model.add(Dense(128, activation='relu', kernel_initializer='random_normal', input_dim=X_train.shape[1]))\n",
    "\n",
    "#Second  Hidden Layer\n",
    "model.add(Dense(64, activation='relu', kernel_initializer='random_normal'))\n",
    "\n",
    "# #Third  Hidden Layer\n",
    "# model.add(Dense(16, activation='relu', kernel_initializer='random_normal'))\n",
    "\n",
    "#Output Layer\n",
    "model.add(Dense(1, activation='sigmoid', kernel_initializer='random_normal'))\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "# there's no easy way to incorporate roc_auc in MLP's training process\n",
    "# so I use accuracy, which can be a proxy of roc_auc, to train models\n",
    "# then use roc_auc to assess the prediction on test set\n",
    "\n",
    "# add early stop to prevent overfitting\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=50)\n",
    "mc = ModelCheckpoint('best_model2.h5', monitor='val_loss', mode='min', verbose=1, save_best_only=True)\n",
    "\n",
    "history = model.fit(X_train, y_train,\n",
    "                    validation_split = 0.15,\n",
    "                    epochs=4000, verbose=0, callbacks=[es, mc])\n",
    "\n",
    "saved_model = load_model('best_model2.h5')\n",
    "\n",
    "_, train_acc = saved_model.evaluate(X_train, y_train, verbose=0)\n",
    "_, test_acc = saved_model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roc_auc on train:  0.8755809082094415\n",
      "roc_auc on test:  0.882683602888821\n"
     ]
    }
   ],
   "source": [
    "prediction_on_X_train = saved_model.predict_proba(X_train)\n",
    "prediction_on_X_train = [x[0] for x in prediction_on_X_train]\n",
    "print('roc_auc on train: ', roc_auc_score(y_train, prediction_on_X_train))\n",
    "\n",
    "prediction_on_X_test = saved_model.predict_proba(X_test_s)\n",
    "prediction_on_X_test = [x[0] for x in prediction_on_X_test]\n",
    "print('roc_auc on test: ', roc_auc_score(y_test, prediction_on_X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Light GBM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train roc_auc:  0.9292614434081056\n",
      "test roc_auc:  0.8705631910049059\n",
      "Wall time: 1.53 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X = final_frame.iloc[:,1:]\n",
    "y = final_frame.iloc[:,0].map(lambda x: 1 if x == True else 0 )\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=33)\n",
    "\n",
    "bst = lgb.LGBMClassifier(boosting_type= 'gbdt',\n",
    "                        objective = 'binary',\n",
    "                        max_depth = 3,\n",
    "                        n_estimators = 1000)\n",
    "\n",
    "bst.fit(X_train.values, y_train.values)\n",
    "\n",
    "prediction_on_X_train = bst.predict_proba(X_train)\n",
    "print('train roc_auc: ', roc_auc_score(y_train, prediction_on_X_train[:,1]))\n",
    "\n",
    "prediction_on_X_test = bst.predict_proba(X_test)\n",
    "print('test roc_auc: ', roc_auc_score(y_test, prediction_on_X_test[:,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combined accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test roc_auc:  0.8812656001815294\n"
     ]
    }
   ],
   "source": [
    "NN_predict = saved_model.predict_proba(X_test_s)\n",
    "bst_predict = bst.predict_proba(X_test)\n",
    "rf_predict = rf_model.predict_proba(X_test)\n",
    "\n",
    "nn_prob = np.array([ x[0] for x in NN_predict])\n",
    "bst_prob = bst_predict[:,1]\n",
    "rf_prob = rf_predict[:,1]\n",
    "\n",
    "avg = (nn_prob+bst_prob+rf_prob)/3\n",
    "avg\n",
    "\n",
    "print('test roc_auc: ', roc_auc_score(y_test, avg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like the class is working!!!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
